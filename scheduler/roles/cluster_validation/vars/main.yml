#  Copyright 2025 Dell Inc. or its subsidiaries. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
---

# Usage: main.yml
input_project_dir: "{{ hostvars['localhost']['input_project_dir'] }}"
omnia_log_path: /var/log/omnia
oim_path: "/root/.ansible/oim/"
ansible_cfg_src: "{{ playbook_dir }}/ansible.cfg"
ansible_cfg_dest:
  - { path: "{{ playbook_dir }}/telemetry/ansible.cfg", log_path: "/var/log/omnia/omnia_telemetry.log", regexp: "/var/log/omnia.log" }
  - { path: "{{ playbook_dir }}/platforms/ansible.cfg", log_path: "/var/log/omnia/omnia_platforms.log", regexp: "/var/log/omnia.log" }

# Usage: include_local_repo_config.yml
local_repo_config_file: "{{ input_project_dir }}/local_repo_config.yml"
local_repo_config_syntax_fail_msg: "Failed. Syntax errors present in local_repo_config.yml. Fix errors and re-run playbook again."

# Usage: fetch_software_config.yml
software_config_json_file: "{{ input_project_dir }}/software_config.json"
local_repo_access_dest_path: "/opt/omnia/provision/local_repo_access.yml"

k8s_version_fail_msg: "Failed, Ensure version of k8s is mentioned in software_config.json"
k8s_packages_file: "{{ input_project_dir }}/config/{{ software_config.cluster_os_type }}/{{ software_config.cluster_os_version }}/k8s.json"
success_msg_ucx_version: "Success. ucx version is mentioned."
fail_msg_ucx_version: "Failed. ucx version is not provided in software_config.json. Please include ucx version in input/software_config.json and rerun the playbook." # noqa: yaml[line-length]
success_msg_openmpi_version: "Success. openmpi version is mentioned."
fail_msg_openmpi_version: "Failed. openmpi version is not provided in software_config.json. Please include openmpi version in input/software_config.json and rerun the playbook." # noqa: yaml[line-length]
compute_os_ubuntu: "ubuntu"
intel_gaudi_input_fail_msg: "Warning, software_config.json does not have the intel software stack. Intel stack will not be configured."
intel_gaudi_repo_fail_msg: "Failed, local_repo.yml is not executed for downloading Intel Gaudi driver packages."
warning_time: 30
offline_intel_directory: "{{ repo_store_path }}/cluster/apt"
intelgaudi_version_warning_msg: "Failed, Intel Gaudi version not found."

# Usage: validate_network_spec.yml
network_spec: "{{ input_project_dir }}/network_spec.yml"
network_spec_syntax_fail_msg: "Failed. Syntax errors present in network_spec.yml. Fix errors and re-run playbook again."
admin_nic_ip_fail_msg: "IP '{{ admin_nic_ip }}' is not assigned to NIC '{{ admin_nic }}'. Please configure the admin IP in OIM and re-run the playbook."
admin_nic_ip_success_msg: "IP '{{ admin_nic_ip }}' is assigned to NIC '{{ admin_nic }}'."

# Usage: include_high_availability_config.yml
ha_config_file: "{{ input_project_dir }}/high_availability_config.yml"
ha_config_syntax_fail_msg: "Failed. Syntax errors present in high_availability_config.yml. Fix errors and re-run playbook again."
ha_vip_address_fail_msg: "When k8s HA is enabled , provide k8s_head_node_ha.virtual_ip_address in high_availability_config.yml"

# Usage: generate_k8s_vars.yml
k8s_var_common_src: "../templates/common_vars_template.j2"
k8s_var_final_src: "../templates/final_template.j2"
k8s_var_dest: "/opt/omnia/kubespray/k8s_all_vars.yml"
file_permission: "0644"
pulp_registry_port: "2225"

# Usage: fetch_omnia_inputs.yml
config_filename: "omnia_config.yml"
min_length: 8
max_length: 30
omnia_config_syntax_fail_msg: "Failed. Syntax errors present in omnia_config.yml. Fix errors and re-run playbook again."
success_msg_k8s_cni: "Kubernetes CNI Validated"
fail_msg_k8s_cni: "Kubernetes CNI not correct."
supported_topology_manager_policy: ['none', 'best-effort', 'restricted', 'single-numa-node']
success_msg_k8s_toplogy_manager_policy: "topology_manager_policy validated"
fail_msg_k8s_toplogy_manager_policy: "topology_manager_policy can either be 'none' or 'best-effort' or 'restricted' or 'single-numa-node' in omnia_config.yml"
supported_topology_manager_scope: ['pod', 'container']
success_msg_k8s_toplogy_manager_scope: "topology_manager_scope validated"
fail_msg_k8s_toplogy_manager_scope: "topology_manager_scope can either be 'pod' or 'container' in omnia_config.yml"
success_msg_pod_external_ip_range: "pod_external_ip_range validated"
fail_msg_pod_external_ip_range: "pod_external_ip_range is not given in correct format in omnia_config.yml"
success_msg_k8s_service_addresses: "k8s_service_addresses validated"
fail_msg_k8s_service_addresses: "k8s_service_addresses value not given in correct format in omnia_config.yml"
success_msg_k8s_pod_network_cidr: "k8s_pod_network_cidr validated"
fail_msg_k8s_pod_network_cidr: "k8s_pod_network_cidr is not given in correct format"
file_perm: '0755'
ldap_required_success_msg: "ldap_required variable successfully validated"
ldap_required_fail_msg: "Failed. ldap_required should be either true or false"
freeipa_required_success_msg: "freeipa_required variable sccessfully validated"
freeipa_required_fail_msg: "Failed. freeipa_required should be either true or false"
ldap_login_failure_msg: "Failed. Both login_node_required and ldap_required cannot be true"
ldap_freeipa_failure_msg: "Failed. Both ldap_required and freeipa_required cannot be true"
enable_omnia_nfs_success_msg: "enable_omnia_nfs successfully validated"
enable_omnia_nfs_fail_msg: "Failed. enable_omnia_nfs should be either true or false"
input_config_failure_msg: "None of the parameters in omnia_config.yml should be empty."
slurm_installation_type_empty_failure_msg: "Slurm Installation type cannot be empty in omnia_config.yml"
slurm_installation_type_wrong_failure_msg: "Slurm Installation Type should be either nfs_share or configless in omnia_config.yml"
restart_services_success_msg: "restart_slurm_services successfully validated"
restart_services_failure_msg: "Failed. restart_slurm_services accepts true or false in omnia_config.yml"

# Usage: fetch_storage_config.yml
storage_config_filename: "storage_config.yml"
storage_config_syntax_fail_msg: "Failed. Syntax errors present in storage_config.yml. Fix errors and re-run playbook again."
nfs_client_params_failure_msg: "nfs_client_params variable can not be kept empty in input/storage_config.yml. It should have atleast one nfs share details."
nfs_client_params_k8s_share_fail_msg: "Exactly one entry should be present in nfs_client_params with k8s_share as true in input/storage_config.yml"
nfs_client_params_k8s_share_success_msg: "Entry found in nfs_client_params with k8s_share as true"
nfs_client_params_slurm_share_fail_msg: "Exactly one entry should be present in nfs_client_params with slurm_share as true in input/storage_config.yml"
nfs_client_params_slurm_share_success_msg: "Entry found in nfs_client_params with slurm_share as true"
nfs_client_params_benchmarks_fail_msg: "Atleast one out of k8s_share or slurm_share should be true in input/storage_config.yml when ucx/openmpi are installed on cluster nodes." # noqa: yaml[line-length]
nfs_client_params_benchmarks_success_msg: "Entry found in nfs_client_params with slurm_share or k8s_share as true"

# Usage: validate_scheduler_type.yml
scheduler_type_success_msg: "scheduler_type successfully validated"
scheduler_type_fail_msg: "Failed. Invalid scheduler_type in omnia_config.yml. To install slurm provide scheduler_type: slurm
To install k8s provide scheduler_type: k8s. To install slurm and k8s provide scheduler_type: slurm,k8s"
install_scheduler_msg: "Installing job scheduler:"

# # Usage: Fetch_software_config.yml
# csi_driver_powerscale_packages_file: >-
#   {{ role_path }}/../../../input/config/{{ software_config.cluster_os_type }}/{{ software_config.cluster_os_version }}/csi_driver_powerscale.json

# Usage: fetch_omnia_inputs.yml
csi_driver_secret_file_path_success_msg: "Success. csi_driver_secret_file_path is valid in omnia_config.yml"
csi_driver_secret_file_path_fail_msg: "Failed. csi_driver_secret_file_path is not valid in omnia_config.yml. Please verify the path."

csi_driver_values_file_path_success_msg: "Success. csi_driver_values_file_path is valid in omnia_config.yml"
csi_driver_values_file_path_fail_msg: "Failed. csi_driver_values_file_path is not valid in omnia_config.yml. Please verify the path."

# Usage: csi_powerscale_driver_input_validation.yml
csi_powerscale_secret_vaultname: ".csi_powerscale_secret_vault"
fail_msg_isilon_clusters: "isilonClusters must be a valid list of powerscale details in secret.yaml file."
fail_msg_cluster_name: "clusterName is not valid. Provide powerscale cluster name in secret.yaml file."
fail_msg_user_name: "userName is not valid. Provide powerscale user name in secret.yaml file."
fail_msg_password: "Password is not valid. Provide powerscale password in secret.yaml file."
fail_msg_endpoint: "Endpoint is not valid. Provide powerscale IP or hostname in secret.yaml file."
fail_msg_endpoint_port: "endpointPort is not valid. Provide valid port number in secret.yaml file."
fail_msg_isdefault: "isDefault value should be true or false in secret.yaml file."
fail_msg_skip_certificate_validation: "skipCertificateValidation must be true in secret.yaml file."
fail_msg_isipath: "isiPath must be a valid Unix absolute path in secret.yaml file."
fail_msg_isi_volume_path_permissions: "isiVolumePathPermissions must be a valid directory permission (example: 0777) in secret.yaml file."
fail_msg_api_call: "Please recheck powerscale username, password, endpoint and endpointPort details provided in secret.yaml and
  values.yaml (if endpointPort is provided only in values.yaml) file. API call to powerscale was not successful"
vault_key_permission: "0644"
