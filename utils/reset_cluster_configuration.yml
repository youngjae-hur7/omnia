#  Copyright 2025 Dell Inc. or its subsidiaries. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
---

- name: Include input project directory
  ansible.builtin.import_playbook: ../utils/include_input_dir.yml

- name: Create kubespray container group
  ansible.builtin.import_playbook: ../utils/create_container_group.yml
  vars:
    omnia_kubespray_group: true
    omnia_provision_group: true
    omnia_provision_validation: true
  tags: always

- name: Warning and User confirmation for removing cluster
  hosts: localhost
  connection: local
  gather_facts: false
  roles:
    - { role: remove_cluster/user_confirmation } # noqa: role-name[path]
    - { role: common }
  tags: always

- name: Host Mapping
  hosts: omnia_provision
  connection: ssh
  roles:
    - servicetag_host_mapping
  tags: always

- name: Proceeding to remove slurm cluster
  hosts: slurm_node, login, slurm_control_node
  connection: ssh
  gather_facts: true
  tags: slurm_cluster
  roles:
    - { role: remove_cluster/remove_slurm_cluster } # noqa: role-name[path]

- name: Proceeding to remove kubernets cluster
  tags: k8s_cluster
  hosts: localhost
  connection: ssh
  gather_facts: false
  roles:
    - { role: remove_cluster/remove_k8s_cluster } # noqa: role-name[path]

- name: Reset kubernetes cluster
  hosts: omnia_kubespray
  vars:
    k8s_nfs_share: "/opt/omnia/kubespray"
    k8s_log_dir: "/opt/omnia/log/kubespray"
    dir_mode: '0755'
  tasks:
    - name: Execute tasks for Reset kubernetes setup
      block:
        - name: Copy reset_k8s_cluste to kubespray nfs share
          ansible.builtin.copy:
            src: "{{ playbook_dir }}/playbooks/reset_k8s_cluster.yml"
            dest: "{{ k8s_nfs_share }}/reset_k8s_cluster.yml"
            mode: "{{ dir_mode }}"

        - name: Ensure log/kubespray directory exists
          ansible.builtin.file:
            path: "{{ k8s_log_dir }}"
            state: directory
            mode: "{{ dir_mode }}"

        - name: Execute ansible-playbook for Kubernetes setup asynchronously
          ansible.builtin.shell:
            cmd: >-
              set -o pipefail &&
              /venv/bin/ansible-playbook {{ k8s_nfs_share }}/reset_k8s_cluster.yml
              -i {{ k8s_nfs_share }}/inv_k8s
              --extra-vars "@{{ k8s_nfs_share }}/k8s_all_vars.yml" -vvv
              | /usr/bin/tee {{ k8s_log_dir }}/k8s_reset_cluster.log
          async: 3600  # Set async timeout (e.g., 1 hour)
          poll: 0  # Non-blocking (continue the playbook without waiting for completion)
          register: k8s_reset_result  # Register the result to capture job ID
          changed_when: false

        - name: Wait for the Kubernetes rest cluster to finish. Logs can be checked at /opt/omnia/log/kubespray/k8s_reset_cluster.log
          ansible.builtin.async_status:
            jid: "{{ k8s_reset_result.ansible_job_id }}"  # Job ID from the previous task
          register: job_result
          until: job_result.finished
          retries: 60  # Retry the task 60 times (10 min total)
          delay: 10  # Wait 10 seconds between retries

# manually removing kube service address from /etc/resolv.conf
- name: Remove kube service address post kubernetes cluster removal
  tags: k8s_cluster
  hosts: kube_node, kube_control_plane, etcd
  connection: ssh
  gather_facts: false
  roles:
    - { role: remove_cluster/post_k8s_cluster_removal } # noqa: role-name[path]

- name: Confirm slurm cluster removal
  hosts: slurm_control_node
  connection: ssh
  gather_facts: false
  tags: slurm_cluster
  roles:
    - { role: remove_cluster/verify_slurm_cluster } # noqa: role-name[path]

- name: Confirm kubernetes cluster removal
  hosts: kube_control_plane
  connection: ssh
  gather_facts: false
  tags: k8s_cluster
  roles:
    - { role: remove_cluster/verify_k8s_cluster } # noqa: role-name[path]
